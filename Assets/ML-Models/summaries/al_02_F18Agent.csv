Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
10000,1.4189383,-0.15679993,999.0,161.04444658507904,161.04444658507904,1.0
20000,1.4189383,-0.12318449,999.0,175.70000467300414,175.70000467300414,1.0
30000,1.4208456,0.45180616,999.0,148.94000417739153,148.94000417739153,1.0
40000,1.420952,0.46285862,999.0,144.54000489488243,144.54000489488243,1.0
50000,1.4205364,0.81643105,999.0,172.76000625267625,172.76000625267625,1.0
60000,1.4204885,0.8406746,999.0,192.1700048804283,192.1700048804283,1.0
70000,1.4199157,1.1017282,999.0,237.0200064897537,237.0200064897537,1.0
80000,1.4198115,1.1441517,999.0,291.68000876903534,291.68000876903534,1.0
90000,1.4185518,1.7937495,999.0,286.6600034236908,286.6600034236908,1.0
100000,1.4182312,1.968573,999.0,312.8600086212158,312.8600086212158,1.0
110000,1.4182053,2.5793946,999.0,325.2800093650818,325.2800093650818,1.0
120000,1.418197,2.8390388,999.0,372.8500122070312,372.8500122070312,1.0
130000,1.4191663,3.7535858,999.0,368.0100124359131,368.0100124359131,1.0
