Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
10000,1.4189383,-0.017572287,999.0,258.1138031747606,258.1138031747606,1.0
20000,1.4189383,-0.026108671,999.0,36.14498541355133,36.14498541355133,1.0
30000,1.4212761,0.25765544,999.0,36.55235352516174,36.55235352516174,1.0
40000,1.4214103,0.2527035,999.0,35.75049879550934,35.75049879550934,1.0
50000,1.4198745,0.59197634,999.0,36.26922891139984,36.26922891139984,1.0
60000,1.4196955,0.63581556,999.0,37.288888454437256,37.288888454437256,1.0
70000,1.418346,0.86733073,999.0,35.82945055961609,35.82945055961609,1.0
80000,1.4180993,0.8940447,999.0,35.970033979415895,35.970033979415895,1.0
90000,1.4164672,1.1056058,999.0,34.27208988666534,34.27208988666534,1.0
100000,1.4160506,1.1787312,999.0,36.480762004852295,36.480762004852295,1.0
110000,1.4141767,1.3978654,999.0,34.9788294672966,34.9788294672966,1.0
120000,1.4135374,1.5003741,999.0,35.730224800109866,35.730224800109866,1.0
130000,1.4128878,1.6744068,999.0,35.18846757411957,35.18846757411957,1.0
140000,1.4126079,1.7620281,999.0,35.595934176445006,35.595934176445006,1.0
