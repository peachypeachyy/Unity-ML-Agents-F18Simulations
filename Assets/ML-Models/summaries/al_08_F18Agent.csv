Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
10000,1.4189383,0.5342741,999.0,172.76180091169147,172.76180091169147,1.0
20000,1.4189383,0.6245075,999.0,42.749808406829835,42.749808406829835,1.0
30000,1.415493,0.48592138,999.0,37.04735562801361,37.04735562801361,1.0
40000,1.4152998,0.42784318,999.0,103.58117669820786,103.58117669820786,1.0
50000,1.4152489,0.26740345,999.0,24.136020135879516,24.136020135879516,1.0
60000,1.4152434,0.49095544,999.0,198.5312263250351,198.5312263250351,1.0
70000,1.4153596,0.7162706,999.0,42.62884755134583,42.62884755134583,1.0
80000,1.4153806,1.0645058,999.0,-372.22006645202634,-372.22006645202634,1.0
90000,1.415272,0.60822994,999.0,-361.65771408081054,-361.65771408081054,1.0
100000,1.4152442,0.348013,999.0,-370.34253883361816,-370.34253883361816,1.0
110000,1.4160105,-0.26785892,999.0,-367.5344413757324,-367.5344413757324,1.0
120000,1.4162705,-0.77472633,999.0,-352.5743724822998,-352.5743724822998,1.0
130000,1.4167908,-1.1853615,999.0,-375.050186920166,-375.050186920166,1.0
140000,1.417015,-1.8334202,999.0,-381.279972076416,-381.279972076416,1.0
